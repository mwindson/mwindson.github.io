{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"Node.js笔记之Stream","slug":"Node-js笔记之Stream","date":"2018-05-24T12:57:11.012Z","updated":"2018-05-26T13:54:31.156Z","comments":true,"path":"2018/05/24/Node-js笔记之Stream/","link":"","permalink":"http://yoursite.com/2018/05/24/Node-js笔记之Stream/","excerpt":"","text":"什么是 stream流（stream）是一种处理数据的思想——数据可以如水流一样流动，可以分段，具有方向。流的思想使它在操作大量数据或连续不断的数据具有优势。 Stream 类别Node.js 中具有四种基本类型的流： Readable - 可读流 Writable - 可写的流 Duplex - 可读写的流 Transform - 特殊的Duplex流，在读写中可以修改和变换数据 Readable 流Readable 流中的数据并不是直接流向消费者，而是先 push 到缓存池中。当数据超过缓存池的阈值标志 highWatermark 时，push 就会失败并返回 false。 Readable 流具有两种状态：flowing 和 pause —— 前者会将数据不断 pipe 给下游流，后者会停止传输数据直到下游流显示的调用 Stream.read()。 Readable 流默认是 pause 的。 flowing mode pause mode 两者状态转换情况： pause -&gt; flowing: 添加 data 事件订阅 调用.resume()转换为 flowing 调用.pipe()将数据传输给 Writable 流 flowing -&gt; pause: 未 pipe 任何流时，调用.pause()暂停 已经 pipe 到其他流上时，需要移除所有 data 订阅事件，并且逐一调用.unpipe()解除。 Writable 流数据流过来的时候，Writable 流内部通过writeOrBuffer来判断将数据是直接提供给数据消费者还是暂时先存放到缓冲区。当写入速度过快或者无消费者时，数据流会进入缓冲池缓存起来。 Duplex 流Duplex 流同时具有可写和可读流，不过两个流之间没有关系，互不干扰。 Transform 流Transform 流同样具有可写和可读的能力。不过可写流会通过一次 transform 函数的转换处理，然后作为可读流，提供给消费者。 pipe.pipe()方法是 stream 中最重要的方法。它提供了数据之间流动/桥接的方法。 具体方法如下： 12// 将一个可读流的数据流向一个可写流Readable.pipe(Writbale) 自定义流的实现Node.js 的stream流模块用于简单的实现流。开发者可以通过 js 的原型继承模式或更简单的构造函数方式来实现自定义流。不同种类的流必须实现相应的方法。 原型继承法1234567891011121314// 需要实现_write方法class myWritableStream extends Writable &#123; constructor(opts) &#123; super(opts) &#125; _write(chunk, encoding, callback) &#123; console.log(chunk.toString().toUpperCase()) callback() &#125;&#125;new myWritableStream().write('test', () =&gt; &#123; console.log('write end')&#125;)// print \"TEST\" 构造函数法123456789101112// transform流需要实现transform方法const myTransform = new Transform(&#123; transform(chunk, encoding, callback) &#123; this.push(chunk.toString().toUpperCase()) callback() &#125;&#125;)myTransform.write('aaa')myTransform.end()myTransform.pipe(process.stdout)// print \"AAA\" BackpressureBackpressure是指在数据流从上游生产者向下游消费者传输的过程中，上游生产速度大于下游消费速度，导致下游的 Buffer 溢出的一种现象。 由于消费者消费速度过慢或生产者生产速度过快， stream 内部可能会产生 Backpressure 现象。 Readable 流当 Readable 流处于 pause mode 或者消费速度比 push 到缓存池的速度慢时，就可能发生 Backpressure 现象。 Writable 流当外部生产者生产速度过快时，Writable 内部的队列池会装满，此时会发生 Backpressure 问题，同时 Writable 流无法再接受生产者数据流。只有当队列得到释放后，Writable 流会触发 drain 事件。 1234567891011121314151617181920212223242526272829303132const Writable = require('stream').Writableconst writer = new Writable(&#123; write(chunk, encoding, callback) &#123; // 延迟callback setTimeout(() =&gt; &#123; callback &amp;&amp; callback() &#125;) &#125;&#125;)writeOneMillionTimes(writer, 'simple', 'utf8', () =&gt; &#123; console.log('end')&#125;)// 写入10000次function writeOneMillionTimes(writer, data, encoding, callback) &#123; // let i = 1000000 改为1万次 let i = 10000 write() function write() &#123; let ok = true while (i-- &gt; 0 &amp;&amp; ok) &#123; // 写入结束时回调 ok = writer.write(data, encoding, i === 0 ? callback : null) &#125; if (i &gt; 0) &#123; // 这里提前停下了，'drain' 事件触发后才可以继续写入 console.log('drain', i) writer.once('drain', write) &#125; &#125;&#125; 结果 1234drain 7268drain 4536drain 1804end Duplex 流Duplex 流是 Readable 流和 Writable 流的组合，因此内部的 Readable 流和 Writable 流的 Backpressure 问题同上。 Tansform 流由于 Transform 流输入与输出是相互关联，内部 Readable 和 Writable 的速度相同，因此内部不存在 Backpressure 问题，而会取决于外部生产者、消费者速度的影响。","categories":[],"tags":[{"name":"js","slug":"js","permalink":"http://yoursite.com/tags/js/"},{"name":"node","slug":"node","permalink":"http://yoursite.com/tags/node/"}]},{"title":"Node.js笔记之cluster","slug":"Node-js笔记之cluster","date":"2018-05-21T11:23:21.224Z","updated":"2018-05-22T13:09:12.743Z","comments":true,"path":"2018/05/21/Node-js笔记之cluster/","link":"","permalink":"http://yoursite.com/2018/05/21/Node-js笔记之cluster/","excerpt":"","text":"由于单个 Node 进程在运行时只能使用其中一个内核，因此简单的 Node 程序无法充分利用多核 CPU。为了解决这个问题，Node 提供了 cluster(集群)API。 cluster 概述cluster提供了简单的创建共享服务器端口的子进程。 12345678910111213141516171819const cluster = require('cluster')const http = require('http')const numCPUs = require('os').cpus().lengthif (cluster.isMaster) &#123; console.log('CPU数', numCPUs) console.log('master is running') // 根据CPU的核数进行子进程的创建 for (let i = 0; i &lt; numCPUs; i++) &#123; cluster.fork() &#125;&#125; else &#123; http .createServer(function(req, res) &#123; res.writeHead(200) res.end(`a worker running in process $&#123;process.pid&#125;`) &#125;) .listen(3000)&#125; 进程创建、销毁进程创建cluster.fork可以创建一个子进程并返回。fork行为可以通过cluster.setupMaster设置并修改。 当子进程被 fork 后，cluster主进程的fork、online事件会依次触发。 进程销毁子进程可以通过disconnect和exit销毁。当子进程退出时，子进程会断开与主进程的IPC管道，然后退出。主进程的disconnect、exit事件会依次触发。 12345678910111213141516171819202122232425262728293031323334const cluster = require('cluster')const http = require('http')const numCPUs = require('os').cpus().lengthif (cluster.isMaster) &#123; console.log('CPU数', numCPUs) console.log('master is running') for (let i = 0; i &lt; numCPUs; i++) &#123; cluster.fork() &#125; cluster.on('fork', worker =&gt; &#123; console.log(`worker $&#123;worker.id&#125; is forked`) &#125;) cluster.on('online', worker =&gt; &#123; console.log(`worker $&#123;worker.id&#125; is running`) &#125;) cluster.on('listening', (worker, address) =&gt; &#123; console.log(`A worker $&#123;worker.id&#125; is now connected to $&#123;address.address&#125;:$&#123;address.port&#125;`) &#125;) cluster.on('disconnect', worker =&gt; &#123; console.log(`worker $&#123;worker.id&#125; has disconnected`) &#125;) cluster.on('exit', function(worker, code, signal) &#123; console.log(`Worker $&#123;worker.id&#125; exit`) &#125;)&#125; else &#123; http .createServer(function(req, res) &#123; res.writeHead(200) res.end(`I am a worker running in process $&#123;process.pid&#125;`) &#125;) .listen(3000) setTimeout(() =&gt; process.disconnect(), 1000)&#125; 进程通信在cluster模块中，父子进程通过IPC的方式进行通信。 父进程消息发送：worker.send()或ChildProcess.send() 父进程消息接受：cluster监听 message 子进程消息发送：process监听 message 父子进程相互通信并且遍历cluster.workers查看运行 worker 的 id 1234567891011121314151617181920212223242526272829const cluster = require('cluster')const http = require('http')const numCPUs = require('os').cpus().lengthif (cluster.isMaster) &#123; console.log('[master] ' + 'start master...') for (let i = 0; i &lt; numCPUs; i++) &#123; const wk = cluster.fork() wk.send('[master] ' + 'hi worker' + wk.id) &#125; cluster.on('message', (worker, msg, handle) =&gt; &#123; console.log(`[worker] worker $&#123;worker.id&#125; : $&#123;msg&#125;`) &#125;) cluster.on('exit', (worker, code, signal) =&gt; &#123; console.log(`[worker] $&#123;worker.id&#125; disconnected`) for (let id in cluster.workers) &#123; console.log(`woker $&#123;id&#125; is working`) &#125; cluster.fork() &#125;)&#125; else &#123; process.on('message', function(msg) &#123; console.log('[worker] ' + msg) process.send('[worker] worker' + cluster.worker.id + ' received!') process.disconnect() &#125;) http.createServer().listen(3000)&#125; 负载均衡cluster模块的调度策略通过cluster.schedulingPolicy来设置。根据Node文档，除Windows外的操作系统中，cluster模块默认使用round-robin算法进行负载均衡。 在Windows系统中需要设置cluster.schedulingPolicy = cluster.SCHED_RR来启用RR算法来负载均衡 pm2PM2是一个用于Node.js应用的进程管理程序，通过`-i选项来设置来开启cluster模式。另外，PM2还具有无间断重启、增加进程个数的优势。1$ pm2 start app.js -i 4","categories":[],"tags":[{"name":"js","slug":"js","permalink":"http://yoursite.com/tags/js/"},{"name":"node","slug":"node","permalink":"http://yoursite.com/tags/node/"}]},{"title":"Node.js笔记之Buffer","slug":"Node-js笔记之Buffer","date":"2018-04-22T10:40:46.000Z","updated":"2018-04-22T13:17:23.789Z","comments":true,"path":"2018/04/22/Node-js笔记之Buffer/","link":"","permalink":"http://yoursite.com/2018/04/22/Node-js笔记之Buffer/","excerpt":"","text":"Buffer概述Buffer类用于在 TCP 流或文件系统操作等场景中处理二进制数据流。其大小会在创建时确定，无法调整。 创建Buffer在V6之后的Node.js中，原有的new Buffer创建方法由于不可靠和不安全，已经被废弃。现有常用的Buffer创建办法包括： Buffer.from() Buffer.alloc() Buffer.allocUnsafe() 1234567891011// buf1: &lt;Buffer 00 00 00 00 00 00 00 00 00 00&gt;const buf1 = Buffer.alloc(10)// buf2: &lt;Buffer 01 01 01 01 01 01 01 01 01 01&gt;const buf2 = Buffer.alloc(10, 1)// buf3 &lt;Buffer 01 02 03 04&gt;const buf3 = Buffer.from([1, 2, 3, 4])// buf4 &lt;Buffer 74 65 73 74&gt;const buf4 = Buffer.from('test')// 由于allocUnsafe方法会导致Buffer含有旧数据，因此结果不确定// 内容必须被初始化，用buf.fill()来初始化const buf5 = Buffer.allocUnsafe(10) Buffer.allocUnsafe() 速度快，但不安全 当使用 Buffer.allocUnsafe() 分配新建的 Buffer 时，当分配的内存小于 4KB 时，默认会从一个单一的预分配的 Buffer 切割出来。 这使得应用程序可以避免垃圾回收机制因创建太多独立分配的 Buffer。 编码Node.js 目前支持的字符编码包括： ‘ascii’ - 仅支持 7 位 ASCII 数据。如果设置去掉高位的话，这种编码是非常快的。 ‘utf8’ - 多字节编码的 Unicode 字符。许多网页和其他文档格式都使用 UTF-8 。 ‘utf16le’ - 2 或 4 个字节，小字节序编码的 Unicode 字符。支持代理对（U+10000 至 U+10FFFF）。 ‘ucs2’ - ‘utf16le’ 的别名。 ‘base64’ - Base64 编码。 ‘latin1’ - 一种把 Buffer 编码成一字节编码的字符串的方式 ‘binary’ - ‘latin1’ 的别名。 ‘hex’ - 将每个字节编码为两个十六进制字符。 字符串进行编码的方式有两种： 通过Buffer.from(string[, encoding])进行 12// buf6 &lt;Buffer 74 65 73 74&gt;const buf6 = Buffer.from('test', 'ascii') 通过buf.toString([encoding[, start[, end]]])进行 123const buf6 = Buffer.from('test')// 74657374buf6.toString('hex') 其他类方法还有一些 Buffer 常用的 API Buffer.isBuffer：判断对象是否为 Buffer Buffer.isEncoding：判断 Buffer 对象编码 实例方法Buffer实例的API和数组有些相似，包括: buf.length：返回 内存为此 Buffer 实例所申请的字节数，并不是 Buffer 实例内容的字节数 buf.indexOf：和数组的 indexOf 类似，返回某字符串、acsii 码或者 buf 在改 buf 中的位置 buf.copy：将一个 buf 的（部分）内容复制到另外一个 buf 中 buf.equals:如果 buf 与 otherBuffer 具有完全相同的字节，则返回 true，否则返回 false buf.toJSON：将buf转换成JSON格式的数据 123const buf6 = Buffer.from('test')console.log(buf6.toJSON())// 输出 &#123; type: 'Buffer', data: [ 116, 101, 115, 116 ] &#125; 一些指定字节格式读写的API 另外，Buffer实例可以用for..of进行遍历。当然，也存在buf.values() 、buf.keys() 和 buf.entries() 方法","categories":[],"tags":[{"name":"js","slug":"js","permalink":"http://yoursite.com/tags/js/"},{"name":"node","slug":"node","permalink":"http://yoursite.com/tags/node/"}]}]}